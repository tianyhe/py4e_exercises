{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 11.1\n",
    "Read through and parse a file with text and numbers. You will extract all the numbers in the file and compute the sum of the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "fname = input('Please enter the name of the files: ')\n",
    "if len(fname) < 1 : fname = 'regex_sum_309952.txt'\n",
    "fhand = open(fname)\n",
    "\n",
    "lst = list()\n",
    "\n",
    "    # Method 1 - list comprehension 1.0\n",
    "for line in fhand :\n",
    "    #line = line.rstrip()\n",
    "    #if not re.search('[0-9]+', line) : continue\n",
    "    lst = lst + re.findall('[0-9]+', line)\n",
    "\n",
    "print(sum(int(num) for num in lst))\n",
    "    # without list comprehension\n",
    "#tot = 0\n",
    "#for num in lst :\n",
    "#    tot = tot + int(num)\n",
    "#print(tot)\n",
    "\n",
    "    #Method 2 - one line of code - list conprehension ultra\n",
    "print( sum( [ int(x) for x in re.findall('[0-9]+',fhand.read()) ] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises 11.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "cmd = 'GET http://data.pr4e.org/intro-short.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "mysock.send(cmd)\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(512)\n",
    "    if len(data) < 1:\n",
    "        break\n",
    "    print(data.decode(),end='')\n",
    "\n",
    "mysock.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 12.1\n",
    "Scraping HTML Data with BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('span')\n",
    "counts = list()\n",
    "for tag in tags:\n",
    "    # Look at the parts of a tag\n",
    "    print('TAG:', tag)\n",
    "    print('Contents:', tag.contents[0])\n",
    "    print('Attrs:', tag.attrs)\n",
    "    # Create a list with the text content (numbers) that found in each tag\n",
    "    counts.append(tag.contents[0])\n",
    "# Convert all the string to integer with in the list and sum it up    \n",
    "print(sum(int(num) for num in counts))\n",
    "\n",
    "\n",
    "# http://py4e-data.dr-chuck.net/comments_42.html\n",
    "# http://py4e-data.dr-chuck.net/comments_309954.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 12.2\n",
    "Following Links in HTML Using BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "if len(url) < 1: url = 'http://py4e-data.dr-chuck.net/known_by_Gretchen.html'\n",
    "html = urllib.request.urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "count = input('Enter count:')\n",
    "count = int(count)\n",
    "pos = input('Enter position:')\n",
    "pos = int(pos)\n",
    "\n",
    "# Retrieve and follow all of the link in desire position\n",
    "print('Retrieving:', url) # Retrieve the input URL\n",
    "time = 0\n",
    "    # 'while Loop' control how many times we repeat the retrieving process\n",
    "while True:\n",
    "    tags = soup('a')\n",
    "    # the 'for loop' control the link's position\n",
    "    for tag in tags[:pos]:\n",
    "        # get the URL at input position\n",
    "        keyurl = tag.get('href', None)\n",
    "    print('Retrieving:', keyurl)\n",
    "    html = urllib.request.urlopen(keyurl, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    time = time +1\n",
    "    if time == count : break\n",
    "\n",
    "\n",
    "# http://www.dr-chuck.com/page1.htm\n",
    "# http://py4e-data.dr-chuck.net/known_by_Fikret.html\n",
    "# http://py4e-data.dr-chuck.net/known_by_Gretchen.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.1\n",
    "Extracting Data from XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import xml.etree.ElementTree as ET\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter location:')\n",
    "if len(url) < 1: url = 'http://py4e-data.dr-chuck.net/comments_309956.xml'\n",
    "print('Retrieving', url)\n",
    "uhand = urllib.request.urlopen(url, context=ctx)\n",
    "\n",
    "data = uhand.read()\n",
    "print('Retrieved', len(data), 'characters')\n",
    "#print(data.decode())\n",
    "tree = ET.fromstring(data)\n",
    "\n",
    "# Retrieve all the tag 'count'\n",
    "counts = tree.findall('./comments/comment/count')\n",
    "print('Count:', len(counts))\n",
    "\n",
    "# Retrieve all of the text under the tag\n",
    "lst = list()\n",
    "i = 0\n",
    "for count in counts:\n",
    "    snum = counts[i].text\n",
    "    lst.append(int(snum))\n",
    "    i = i + 1\n",
    "#print(lst)\n",
    "# Sum up all the value in the list\n",
    "print('Sum:', sum(lst))    \n",
    "\n",
    "\n",
    "\n",
    "# http://py4e-data.dr-chuck.net/comments_42.xml\n",
    "# http://py4e-data.dr-chuck.net/comments_309956.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.2\n",
    "Extracting Data from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import json\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter location: ')\n",
    "if len(url) < 1: url = 'http://py4e-data.dr-chuck.net/comments_309957.json'\n",
    "print('Retrieving', url)\n",
    "uhand = urllib.request.urlopen(url, context=ctx)\n",
    "\n",
    "data = uhand.read().decode()\n",
    "print('Retrieved', len(data), 'characters')\n",
    "\n",
    "try:\n",
    "    js = json.loads(data)\n",
    "except:\n",
    "    js = None\n",
    "    print('==== Failure To Retrieve ====')\n",
    "    print(data)\n",
    "\n",
    "#print(json.dumps(js, indent=4))\n",
    "\n",
    "# Count how many object within the object 'comments'\n",
    "print(len(js['comments']))\n",
    "\n",
    "counts = list()\n",
    "i = 0\n",
    "for count in js['comments']:\n",
    "    count = js['comments'][i]['count']\n",
    "    counts.append(int(count))\n",
    "    i = i + 1\n",
    "#print(counts)\n",
    "\n",
    "# Sum up all the value in the list\n",
    "print('Sum:', sum(counts))  \n",
    "\n",
    "\n",
    "\n",
    "# http://py4e-data.dr-chuck.net/comments_42.json \n",
    "# http://py4e-data.dr-chuck.net/comments_309957.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13.3\n",
    "Using the GeoJSON API - Use a GeoLocation lookup API modelled after the Google API to look up some universities and parse the returned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import json\n",
    "import ssl\n",
    "\n",
    "api_key = False\n",
    "# If you have a Google Places API key, enter it here\n",
    "# api_key = 'AIzaSy___IDByT70'\n",
    "# https://developers.google.com/maps/documentation/geocoding/intro\n",
    "\n",
    "if api_key is False:\n",
    "    api_key = 42\n",
    "    serviceurl = 'http://py4e-data.dr-chuck.net/json?'\n",
    "else :\n",
    "    serviceurl = 'https://maps.googleapis.com/maps/api/geocode/json?'\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "while True:\n",
    "    address = input('Enter location: ')\n",
    "    if len(address) < 1: break\n",
    "\n",
    "    # encode the user' input from Unicode to UTF-8 and add it to API\n",
    "    parms = dict()\n",
    "    parms['address'] = address\n",
    "    if api_key is not False: parms['key'] = api_key\n",
    "    url = serviceurl + urllib.parse.urlencode(parms)\n",
    "    # Read and decode the files from python\n",
    "    print('Retrieving', url)\n",
    "    uhand = urllib.request.urlopen(url, context=ctx)\n",
    "    data = uhand.read().decode()\n",
    "    print('Retrieved', len(data), 'characters')\n",
    "    # Covert it to JSON for parasing \n",
    "    try:\n",
    "        js = json.loads(data)\n",
    "    except:\n",
    "        js = None\n",
    "    # Debugging \n",
    "    if not js or 'status' not in js or js['status'] != 'OK':\n",
    "        print('==== Failure To Retrieve ====')\n",
    "        print(data)\n",
    "        continue\n",
    "\n",
    "    #print(json.dumps(js, indent=4))\n",
    "\n",
    "    # Find the object dictionary and retrieve its value\n",
    "    place_id = js['results'][0]['place_id']\n",
    "    print('Place id', place_id)    \n",
    "\n",
    "\n",
    "# South Federal University\n",
    "# UW Madison"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}